{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The objective of this notebook is to showcase the usage of the [___financial-fraud-training___ container](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/cugraph/containers/financial-fraud-training) and how to deploy the produced trained models on [NVIDIA Dynamo-Triton](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver).\n",
    "- We use [IBM TabFormer](https://github.com/IBM/TabFormer) as an example dataset and the dataset is preprocess before model training\n",
    "\n",
    "NOTE:\n",
    "* The preprocessing code is written specifically for the TabFormer dataset and will not work with other datasets.\n",
    "* Additionally, a familiarity with [Jupyter](https://docs.jupyter.org/en/latest/what_is_jupyter.html) is assumed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup (Sagemaker studio)\n",
    "This Notebook is designed to work in a Sagemaker studio notebook and we have to run the following commands to make our conda environment function within Sagemaker\n",
    "\n",
    "Please create a Conda environment and add that to the notebook - See the [README](../README.md) file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def execute_with_progress(command):\n",
    "    cmd = f\"conda install -y {package}\"\n",
    "    process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, \n",
    "                              stderr=subprocess.STDOUT, universal_newlines=True)\n",
    "    \n",
    "    lines = []\n",
    "    for line in process.stdout:\n",
    "        lines.append(line.strip())\n",
    "        \n",
    "        # Show only relevant lines\n",
    "        if any(keyword in line.lower() for keyword in ['installing', 'downloading', 'extracting', 'complete']):\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Status: {line.strip()}\")\n",
    "    \n",
    "    return process.wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_with_progress(\"conda env create -f ~/sample-financial-fraud-detection-with-nvidia/conda/notebook_env.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_with_progress(\"conda install -y ipykernel --name fraud_blueprint_env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_with_progress(\"python -m ipykernel install --user --name fraud_blueprint_env --display-name 'fraud_blueprint_env'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activate the env in the kernel\n",
    "\n",
    "Now choose the `fraud_blueprint_env` kernel from within jupyterlab for this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the AWS Environment variables from the infrastructure stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssm_client = boto3.client('ssm')\n",
    "cfn_client = boto3.client('cloudformation')\n",
    "elb_client = boto3.client('elb')\n",
    "\n",
    "def get_cfn_output(stack_name, output_key):\n",
    "    response = cfn_client.describe_stacks(StackName=stack_name)\n",
    "    outputs = response['Stacks'][0]['Outputs']\n",
    "    for output in outputs:\n",
    "        if output['OutputKey'] == output_key:\n",
    "            return output['OutputValue']\n",
    "    return None\n",
    "\n",
    "def get_inference_host():\n",
    "    response = elb_client.describe_load_balancers()['LoadBalancerDescriptions']\n",
    "\n",
    "    for elb in response:\n",
    "        if elb['LoadBalancerName'].startswith('k8s-triton'):\n",
    "            return elb['DNSName']\n",
    "\n",
    "bucket_name = get_cfn_output(\"NvidiaFraudDetectionBlueprintModelExtractor\", \"SourceBucketName\")\n",
    "\n",
    "sagemaker_training_role = get_cfn_output(\"NvidiaFraudDetectionTrainingRole\", \"SageMakerRoleArn\")\n",
    "\n",
    "training_repo = get_cfn_output(\"NvidiaFraudDetectionTrainingImageRepo\", \"TrainingImageRepo\")\n",
    "\n",
    "inference_host = get_inference_host()\n",
    "\n",
    "print(f\"Bucket Name: {bucket_name}\")\n",
    "print(f\"Training Role: {sagemaker_training_role}\")\n",
    "print(f\"Training Repo: {training_repo}\")\n",
    "print(f\"Inference Host: {inference_host}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Step 1: Get and Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Local\n",
    "1. Download the dataset: https://ibm.ent.box.com/v/tabformer-data/folder/130747715605\n",
    "2. untar and uncompreess the file: `tar -xvzf ./transactions.tgz`\n",
    "3. Put card_transaction.v1.csv in in the `data/TabFormer/raw` folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Brev \n",
    "1. Download the dataset: https://ibm.ent.box.com/v/tabformer-data/folder/130747715605\n",
    "2. In the Jupyter notebook window, use the \"File Browser\" section to the data/Tabformer/raw folder\n",
    "3. Drag-and-drop the \"transactions.tgz\" file into the folder\n",
    "    - There is also an \"upload\" option that displays a file selector\n",
    "    - Please wait for the upload to finish, it could take a while, by lookign at the status indocator at the bottom of the window\n",
    "4. Now uncompress and untar by running the following command\n",
    "    - Note: if somethign goes wrong you will need to delete the file rather than trying to overwrite it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that the compressed file was uploaded successfully - the size should be 266M\n",
    "!ls -lh ../data/TabFormer/raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncompress/untar the file\n",
    "!tar xvzf ../data/TabFormer/raw/transactions.tgz -C ../data/TabFormer/raw/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__If__ drag-and-drop is not working, please run the [Download TabFormer](./extra/download-tabformer.ipynb) notebook is the \"extra\" folder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data folder structure\n",
    "The goal is to produce the following structure\n",
    "\n",
    "```\n",
    ".\n",
    "    data\n",
    "    └── TabFormer\n",
    "        └── raw\n",
    "            └── card_transaction.v1.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the raw data is placed as described above, set the path to the TabFormer directory\n",
    "\n",
    "# Change this path to point to TabFormer data\n",
    "data_root_dir = os.path.abspath('../data/TabFormer/') \n",
    "# Change this path to the directory where you want to save your model\n",
    "model_output_dir = os.path.join(data_root_dir, 'trained_models')\n",
    "\n",
    "# Path to save the trained model\n",
    "os.makedirs(model_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define python function to print directory tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(directory, prefix=\"\"):\n",
    "    \"\"\"Recursively prints the directory tree starting at 'directory'.\"\"\"\n",
    "    # Retrieve a sorted list of entries in the directory\n",
    "    entries = sorted(os.listdir(directory))\n",
    "    entries_count = len(entries)\n",
    "    \n",
    "    for index, entry in enumerate(entries):\n",
    "        path = os.path.join(directory, entry)\n",
    "        # Determine the branch connector\n",
    "        if index == entries_count - 1:\n",
    "            connector = \"└── \"\n",
    "            extension = \"    \"\n",
    "        else:\n",
    "            connector = \"├── \"\n",
    "            extension = \"│   \"\n",
    "        \n",
    "        print(prefix + connector + entry)\n",
    "        \n",
    "        # If the entry is a directory, recursively print its contents\n",
    "        if os.path.isdir(path):\n",
    "            print_tree(path, prefix + extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the raw data has been placed properly\n",
    "print_tree(data_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 2: Preprocess the data \n",
    "- Import the Python function for preprocessing the TabFormer data\n",
    "- Call `preprocess_TabFormer` function to prepare the data\n",
    "\n",
    "NOTE: The preprocessing can takes a few minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the \"src\" directory to the search path\n",
    "src_dir = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "sys.path.insert(0, src_dir)\n",
    "\n",
    "# should be able to import from \"src\" folder now\n",
    "from preprocess_TabFormer_lp import preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "user_mask_map, mx_mask_map, tx_mask_map = preprocess_data(data_root_dir)\n",
    "\n",
    "# this will output status as it correlates different attributes with target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tree(data_root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../data/TabFormer/gnn/nodes/offset_range_of_training_node.json to s3://ml-on-containers/data/gnn/nodes/offset_range_of_training_node.json\n",
      "upload: ../data/TabFormer/gnn/nodes/node_label.csv to s3://ml-on-containers/data/gnn/nodes/node_label.csv\n",
      "upload: ../data/TabFormer/gnn/edges/node_to_node.csv to s3://ml-on-containers/data/gnn/edges/node_to_node.csv\n",
      "upload: ../data/TabFormer/gnn/test_gnn/edges/node_to_node.csv to s3://ml-on-containers/data/gnn/test_gnn/edges/node_to_node.csv\n",
      "upload: ../data/TabFormer/gnn/test_gnn/nodes/node_label.csv to s3://ml-on-containers/data/gnn/test_gnn/nodes/node_label.csv\n",
      "upload: ../data/TabFormer/xgb/test.csv to s3://ml-on-containers/data/xgb/test.csv\n",
      "upload: ../data/TabFormer/gnn/nodes/node.csv to s3://ml-on-containers/data/gnn/nodes/node.csv\n",
      "upload: ../data/TabFormer/xgb/validation.csv to s3://ml-on-containers/data/xgb/validation.csv\n",
      "upload: ../data/TabFormer/gnn/test_gnn/nodes/node.csv to s3://ml-on-containers/data/gnn/test_gnn/nodes/node.csv\n",
      "upload: ../data/TabFormer/xgb/training.csv to s3://ml-on-containers/data/xgb/training.csv\n"
     ]
    }
   ],
   "source": [
    "# copy data to S3 to get pulled during training\n",
    "! aws s3 sync \"../data/TabFormer/\" s3://$bucket_name/data/ --exclude \"*/test_gnn\" --force --delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Step 3:  Now train the model using the financial-fraud-training container\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training configuration file\n",
    "NOTE: Training configuration file must conform to schema defined in docs (to be updated.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Important: Models and configuration files needed for deployment using NVIDIA Dynamo-Triton will be saved in model-repository under the folder that is mounted in /trained_models inside the container__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = {\n",
    "  \"paths\": {\n",
    "        \"data_dir\": \"/opt/ml/input/data/data\",\n",
    "        \"output_dir\": \"/opt/ml/model\"\n",
    "  },\n",
    "\n",
    "  \"models\": [\n",
    "    {\n",
    "      \"kind\": \"GNN_XGBoost\",\n",
    "      \"gpu\": \"single\",\n",
    "      \"hyperparameters\": {\n",
    "        \"gnn\":{\n",
    "          \"hidden_channels\": 32,\n",
    "          \"n_hops\": 2,\n",
    "          \"layer\": \"SAGEConv\",\n",
    "          \"dropout_prob\": 0.1,\n",
    "          \"batch_size\": 4096,\n",
    "          \"fan_out\": 10,\n",
    "          \"num_epochs\": 1\n",
    "        },\n",
    "        \"xgb\": {\n",
    "          \"max_depth\": 6,\n",
    "          \"learning_rate\": 0.2,\n",
    "          \"num_parallel_tree\": 3,\n",
    "          \"num_boost_round\": 512,\n",
    "          \"gamma\": 0.0\n",
    "        }\n",
    "\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the training configuration as a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config_file_name = 'training_config.json'\n",
    "\n",
    "with open(os.path.join(training_config_file_name), 'w') as json_file:\n",
    "    json.dump(training_config, json_file, indent=4)\n",
    "\n",
    "# clone config to S3\n",
    "! aws s3 cp ./training_config.json s3://$bucket_name/config/training_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model using financial_fraud_training container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the training in Sagemaker\n",
    "# define training job\n",
    "training_job_name = \"fraud-detection-gnn-\" + str(int(time.time()))\n",
    "\n",
    "# send training job to parameter store\n",
    "ssm_client = boto3.client(\"ssm\")\n",
    "\n",
    "ssm_client.put_parameter(Name=\"/triton/model\", Value=training_job_name, Overwrite=True)\n",
    "\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "response = sagemaker_client.create_training_job(\n",
    "    TrainingJobName=training_job_name,\n",
    "    RoleArn=sagemaker_training_role,\n",
    "    AlgorithmSpecification={\n",
    "        'TrainingImage': f\"{training_image}:latest\",\n",
    "        'TrainingInputMode': 'File',\n",
    "        'ContainerEntrypoint': [\n",
    "            'torchrun',\n",
    "            '--standalone',\n",
    "            '--nnodes=1',\n",
    "            '--nproc_per_node=1',\n",
    "            'main.py',\n",
    "            '--config',\n",
    "            \"/opt/ml/input/data/config/training_config.json\"\n",
    "        ]\n",
    "    },\n",
    "    InputDataConfig=[\n",
    "        {\n",
    "            'ChannelName': 'data',\n",
    "            'DataSource': {\n",
    "                'S3DataSource': {\n",
    "                    'S3DataType': 'S3Prefix',\n",
    "                    'S3Uri': f's3://{bucket_name}/data/gnn/',\n",
    "                    'S3DataDistributionType': 'FullyReplicated'\n",
    "                }\n",
    "            },\n",
    "            'ContentType': 'application/x-directory',\n",
    "            'InputMode': 'File',\n",
    "            'CompressionType': 'None'\n",
    "        },\n",
    "        {\n",
    "            'ChannelName': 'config',\n",
    "            'DataSource': {\n",
    "                'S3DataSource': {\n",
    "                    'S3DataType': 'S3Prefix',\n",
    "                    'S3Uri': f's3://{bucket_name}/config/',\n",
    "                    'S3DataDistributionType': 'FullyReplicated',\n",
    "                }\n",
    "            },\n",
    "            'ContentType': 'application/x-directory',\n",
    "            'InputMode': 'File',\n",
    "            'CompressionType': 'None'\n",
    "        }\n",
    "    ],\n",
    "    OutputDataConfig={\n",
    "        'S3OutputPath': f's3://{bucket_name}/output/'\n",
    "    },\n",
    "    ResourceConfig={\n",
    "        'InstanceType': 'ml.g5.xlarge',\n",
    "        'InstanceCount': 1,\n",
    "        'VolumeSizeInGB': 30\n",
    "    },\n",
    "    StoppingCondition={\n",
    "        'MaxRuntimeInSeconds': 86400\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure that the training job succeeds\n",
    "According to the training configuration file defined earlier, if the training runs successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "def poll_training_status(job_name):\n",
    "    sagemaker_client = boto3.client('sagemaker')\n",
    "    \n",
    "    while True:\n",
    "        response = sagemaker_client.describe_training_job(TrainingJobName=job_name)\n",
    "        status = response['TrainingJobStatus']\n",
    "        \n",
    "        # Clear previous output\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # Print current status with timestamp\n",
    "        print(f\"Job: {job_name}\")\n",
    "        print(f\"Status: {status}\")\n",
    "        print(f\"Last checked: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        if status in ['Completed', 'Failed', 'Stopped']:\n",
    "            break\n",
    "            \n",
    "        # Wait before next check\n",
    "        time.sleep(30)\n",
    "    \n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job: new-nim-1755615683\n",
      "Status: Completed\n",
      "Last checked: 2025-08-19 15:16:31\n"
     ]
    }
   ],
   "source": [
    "final_status = poll_training_status(training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Step 4:  Serve your python backend model using NVIDIA Dynamo-Triton\n",
    "__!Important__: Change MODEL_REPO_PATH to point to `{model_output_dir}` / `python_backend_model_repository` if you used a different path in your training configuration file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install NVIDIA Dynamo-Triton Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'tritonclient[all]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.grpc as triton_grpc\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient import utils as triton_utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Replace HOST with the actual URL where your NVIDIA Dynamo-Triton server is hosted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = inference_host\n",
    "HTTP_PORT = 8005\n",
    "GRPC_PORT = 8006\n",
    "METRICS_PORT = 8007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serve your models with NVIDIA Dynamo-Triton\n",
    "\n",
    "With the infrastructure repo deployed, we have a Lambda function waiting for the training job to complete and for the models to be output to `s3://ml-on-containers-<accountnumber>/output` and then, they'll get extracted to a different bucket to be served by the inference host setup by our infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URLs for GRPC and HTTP request to the inference server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_grpc = triton_grpc.InferenceServerClient(url=f'{HOST}:{GRPC_PORT}')\n",
    "client_http = httpclient.InferenceServerClient(url=f'{HOST}:{HTTP_PORT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for NVIDIA Dynamo-Triton to install packages and come online\n",
    "**NOTE**: This cell can take a few minutes to execute.\n",
    " If the following cell keeps running even after you see `Started HTTPService at {HOST}:{HTTP_PORT}` in the log, you can interrupt the execution of this cell and continue from the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "while True:\n",
    "    client_grpc = triton_grpc.InferenceServerClient(url=f'{HOST}:{GRPC_PORT}')\n",
    "    try:\n",
    "        if client_grpc.is_server_ready():\n",
    "            break\n",
    "    except triton_utils.InferenceServerException as e:\n",
    "        pass\n",
    "    try:\n",
    "        # Run the docker logs command with the --tail option\n",
    "        output = \"TODO: Figure out what's going on here and how to display container logs from k8s inference host\"\n",
    "        print(output.decode(\"utf-8\"))\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error retrieving logs:\", e)\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if NVIDIA Dynamo-Triton is running properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Figure out how to demonstrate that the pod is online from our container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here’s an example of how to prepare data for inference, using random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tritonclient.http import InferenceServerClient, InferInput, InferRequestedOutput\n",
    "\n",
    "def make_example_request():\n",
    "    # -- example sizes --\n",
    "    num_merchants = 5\n",
    "    num_users   = 7\n",
    "    num_edges   = 3\n",
    "    merchant_feature_dim = 24\n",
    "    user_feature_dim = 13\n",
    "    user_to_merchant_feature_dim = 38\n",
    "\n",
    "    # -- 1) features --\n",
    "    x_merchant = np.random.randn(num_merchants, merchant_feature_dim).astype(np.float32)\n",
    "    x_user   = np.random.randn(num_users, user_feature_dim).astype(np.float32)\n",
    "\n",
    "    # -- 2) shap flag and masks --\n",
    "    compute_shap          = np.array([True], dtype=np.bool_)\n",
    "    feature_mask_merchant   = np.random.randint(0,2, size=(merchant_feature_dim,), dtype=np.int32)\n",
    "    feature_mask_user     = np.random.randint(0,2, size=(user_feature_dim,), dtype=np.int32)\n",
    "\n",
    "    # -- 3) edges: index [2, num_edges] and attributes [num_edges,user_to_merchant_feature_dim] --\n",
    "    edge_index_user_to_merchant = np.vstack([\n",
    "        np.random.randint(0, num_users,   size=(num_edges,)),\n",
    "        np.random.randint(0, num_merchants, size=(num_edges,))\n",
    "    ]).astype(np.int64)\n",
    "    \n",
    "    edge_attr_user_to_merchant = np.random.randn(num_edges, user_to_merchant_feature_dim).astype(np.float32)\n",
    "\n",
    "    feature_mask_user_to_merchant =  np.random.randint(0,2, size=(user_to_merchant_feature_dim,), dtype=np.int32)\n",
    "\n",
    "    return {\n",
    "        \"x_merchant\": x_merchant,\n",
    "        \"x_user\": x_user,\n",
    "        \"COMPUTE_SHAP\": compute_shap,\n",
    "        \"feature_mask_merchant\": feature_mask_merchant,\n",
    "        \"feature_mask_user\": feature_mask_user,\n",
    "        \"edge_index_user_to_merchant\": edge_index_user_to_merchant,\n",
    "        \"edge_attr_user_to_merchant\": edge_attr_user_to_merchant,\n",
    "        \"edge_feature_mask_user_to_merchant\": feature_mask_user_to_merchant\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_and_send_inference_request(data):\n",
    "\n",
    "    # Connect to Triton\n",
    "    client = httpclient.InferenceServerClient(url=f'{HOST}:{HTTP_PORT}')\n",
    "\n",
    "    # Prepare Inputs\n",
    "\n",
    "    inputs = []\n",
    "    def _add_input(name, arr, dtype):\n",
    "        inp = InferInput(name, arr.shape, datatype=dtype)\n",
    "        inp.set_data_from_numpy(arr)\n",
    "        inputs.append(inp)\n",
    "\n",
    "    for key, value in data.items():\n",
    "        if key.startswith(\"x_\"):\n",
    "            dtype = \"FP32\"\n",
    "        elif key.startswith(\"feature_mask_\"):\n",
    "            dtype = \"INT32\"\n",
    "        elif key.startswith(\"edge_feature_mask_\"):\n",
    "            dtype = \"INT32\"            \n",
    "        elif key.startswith(\"edge_index_\"):\n",
    "            dtype = \"INT64\"\n",
    "        elif key.startswith(\"edge_attr_\"):\n",
    "            dtype = \"FP32\"\n",
    "        elif key == \"COMPUTE_SHAP\":\n",
    "            dtype = \"BOOL\"\n",
    "        else:\n",
    "            continue  # skip things we don't care about\n",
    "\n",
    "        _add_input(key, value, dtype)\n",
    "\n",
    "\n",
    "    # Outputs\n",
    "\n",
    "    outputs = [InferRequestedOutput(\"PREDICTION\")]\n",
    "\n",
    "    for key in data:\n",
    "        if key.startswith(\"x_\"):\n",
    "            node = key[len(\"x_\"):]  # extract node name\n",
    "            outputs.append(InferRequestedOutput(f\"shap_values_{node}\"))\n",
    "        elif key.startswith(\"edge_attr_\"):\n",
    "            edge_name = key[len(\"edge_attr_\"):]  # extract edge name\n",
    "            outputs.append(InferRequestedOutput(f\"shap_values_{edge_name}\"))\n",
    "    \n",
    "    # Send request\n",
    "\n",
    "    model_name=\"prediction_and_shapley\"\n",
    "    response = client.infer(\n",
    "        model_name,\n",
    "        inputs=inputs,\n",
    "        request_id=str(1),\n",
    "        outputs=outputs,\n",
    "        timeout= 3000\n",
    "    )\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    # always include prediction\n",
    "    result[\"PREDICTION\"] = response.as_numpy(\"PREDICTION\")\n",
    "\n",
    "    # add shap values\n",
    "    for key in data:\n",
    "        if key.startswith(\"x_\"):\n",
    "            node = key[len(\"x_\"):]  # e.g. \"merchant\", \"user\"\n",
    "            result[f\"shap_values_{node}\"] = response.as_numpy(f\"shap_values_{node}\")\n",
    "        if key.startswith(\"edge_attr_\"):\n",
    "            edge_name = key[len(\"edge_attr_\"):]  # e.g. (\"user\" \"to\"  \"merchant\")\n",
    "            result[f\"shap_values_{edge_name}\"] = response.as_numpy(f\"shap_values_{edge_name}\")\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction without computing Shapley values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read preprocessed input transactions to send query to NVIDIA Dynamo-Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_hetero_graph(gnn_data_dir):\n",
    "    \"\"\"\n",
    "    Reads:\n",
    "      - All node CSVs from nodes/, plus their matching feature masks (<node>_feature_mask.csv)\n",
    "        If missing, a mask of all ones is created (np.int32).\n",
    "      - All edge CSVs from edges/:\n",
    "          base        -> edge_index_<edge> (np.int64)\n",
    "          *_attr.csv  -> edge_attr_<edge>  (np.float32)\n",
    "          *_label.csv -> exactly one -> edge_label_<edge> (DataFrame)\n",
    "    \"\"\"\n",
    "    base = os.path.join(gnn_data_dir, \"test_gnn\")\n",
    "    nodes_dir = os.path.join(base, \"nodes\")\n",
    "    edges_dir = os.path.join(base, \"edges\")\n",
    "\n",
    "    out = {}\n",
    "    node_feature_mask = {}\n",
    "\n",
    "    # --- Nodes: every CSV becomes x_<node>; also read/create feature_mask_<node> ---\n",
    "    if os.path.isdir(nodes_dir):\n",
    "        for fname in os.listdir(nodes_dir):\n",
    "            if fname.lower().endswith(\".csv\") and not fname.lower().endswith(\"_feature_mask.csv\"):\n",
    "                node_name = fname[:-len(\".csv\")]\n",
    "                node_path = os.path.join(nodes_dir, fname)\n",
    "                node_df = pd.read_csv(node_path)\n",
    "                out[f\"x_{node_name}\"] = node_df.to_numpy(dtype=np.float32)\n",
    "\n",
    "                # feature mask file (optional)\n",
    "                mask_fname = f\"{node_name}_feature_mask.csv\"\n",
    "                mask_path = os.path.join(nodes_dir, mask_fname)\n",
    "                if os.path.exists(mask_path):\n",
    "                    mask_df = pd.read_csv(mask_path, header=None)\n",
    "                    node_feature_mask[node_name] = mask_df\n",
    "                    feature_mask = mask_df.to_numpy(dtype=np.int32).ravel()\n",
    "                else:\n",
    "                    # create a must with all zeros\n",
    "                    feature_mask = np.zeros(node_df.shape[1], dtype=np.int32)\n",
    "                out[f\"feature_mask_{node_name}\"] = feature_mask\n",
    "\n",
    "    # --- Edges: group into base, attr, label by filename suffix ---\n",
    "    base_edges = {}\n",
    "    edge_attrs = {}\n",
    "    edge_labels = {}\n",
    "    edge_feature_mask = {}\n",
    "\n",
    "    if os.path.isdir(edges_dir):\n",
    "        for fname in os.listdir(edges_dir):\n",
    "            if not fname.lower().endswith(\".csv\"):\n",
    "                continue\n",
    "            path = os.path.join(edges_dir, fname)\n",
    "            lower = fname.lower()\n",
    "            if lower.endswith(\"_attr.csv\"):\n",
    "                edge_name = fname[:-len(\"_attr.csv\")]\n",
    "                edge_attrs[edge_name] = pd.read_csv(path) #, header=None)\n",
    "            elif lower.endswith(\"_label.csv\"):\n",
    "                edge_name = fname[:-len(\"_label.csv\")]\n",
    "                edge_labels[edge_name] = pd.read_csv(path)\n",
    "            elif lower.endswith(\"_feature_mask.csv\"):\n",
    "                edge_name = fname[:-len(\"_feature_mask.csv\")]\n",
    "                edge_feature_mask[edge_name] = pd.read_csv(path, header=None)\n",
    "            else:\n",
    "                edge_name = fname[:-len(\".csv\")]\n",
    "                base_edges[edge_name] = pd.read_csv(path) #, header=None)\n",
    "\n",
    "\n",
    "\n",
    "    # Enforce: only one label file total\n",
    "    if len(edge_labels) == 0:\n",
    "        raise FileNotFoundError(\"No '*_label.csv' found in edges/. Exactly one label file is required.\")\n",
    "    if len(edge_labels) > 1:\n",
    "        raise ValueError(f\"Found multiple label files: {list(edge_labels.keys())}. Exactly one is allowed.\")\n",
    "\n",
    "    # Build output keys for edges\n",
    "    for edge_name, df in base_edges.items():\n",
    "        out[f\"edge_index_{edge_name}\"] = df.to_numpy(dtype=np.int64).T\n",
    "        if edge_name in edge_attrs:\n",
    "            out[f\"edge_attr_{edge_name}\"] = edge_attrs[edge_name].to_numpy(dtype=np.float32)\n",
    "        if edge_name in edge_feature_mask:\n",
    "            out[f\"edge_feature_mask_{edge_name}\"] = edge_feature_mask[edge_name].to_numpy(dtype=np.int32).ravel()\n",
    "        else:\n",
    "            # create a must with all zeros\n",
    "            out[f\"edge_feature_mask_{edge_name}\"] = np.zeros(edge_attrs[edge_name].shape[1], dtype=np.int32)\n",
    "\n",
    "        \n",
    "\n",
    "    # Add the single label file (kept as DataFrame)\n",
    "    (label_edge_name, label_df), = edge_labels.items()\n",
    "    out[f\"edge_label_{label_edge_name}\"] = label_df\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_hetero_graph(gnn_data_dir)\n",
    "compute_shap = False\n",
    "result =  prepare_and_send_inference_request(test_data | {\"COMPUTE_SHAP\": np.array([compute_shap], dtype=np.bool_)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['PREDICTION']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_score_for_batch(y, predictions, decision_threshold = 0.5):\n",
    "    # Apply threshold\n",
    "    y_pred = (predictions > decision_threshold).astype(int)\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred, zero_division=0)\n",
    "    recall = recall_score(y, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y, y_pred, zero_division=0)\n",
    "\n",
    "    # Confusion matrix\n",
    "    classes = ['Non-Fraud', 'Fraud']\n",
    "    columns = pd.MultiIndex.from_product([[\"Predicted\"], classes])\n",
    "    index = pd.MultiIndex.from_product([[\"Actual\"], classes])\n",
    "\n",
    "    conf_mat = confusion_matrix(y, y_pred)\n",
    "    cm_df = pd.DataFrame(conf_mat, index=index, columns=columns)\n",
    "    print(cm_df)\n",
    "\n",
    "    # Plot the confusion matrix directly\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(\n",
    "        y, y_pred, display_labels=classes\n",
    "    )\n",
    "    disp.ax_.set_title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary\n",
    "    print(\"----Summary---\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scores on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision threshold to flag a transaction as fraud\n",
    "#Change to trade-off precision and recall\n",
    "decision_threshold = 0.5\n",
    "y = test_data['edge_label_user_to_merchant'].to_numpy(dtype=np.int32)\n",
    "compute_score_for_batch(y, result['PREDICTION'], decision_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Shapley values of different features for a transaction\n",
    "NOTE: Shapely computation is very expensive, it will only compute shap values for first transaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shapley values for different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data =  load_hetero_graph(gnn_data_dir)\n",
    "compute_shap = True\n",
    "result_with_shap = prepare_and_send_inference_request(test_data | {\"COMPUTE_SHAP\": np.array([compute_shap], dtype=np.bool_)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in result_with_shap:\n",
    "    if key.startswith('shap_'):\n",
    "        print(f'{key} : {result_with_shap[key]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_masks = {\n",
    "    'user': user_mask_map,\n",
    "    'merchant': mx_mask_map,\n",
    "    'user_to_merchant': tx_mask_map\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in feature_masks:\n",
    "    shap_values = result_with_shap[f'shap_values_{key}']\n",
    "    min_idx = min(feature_masks[key].values())\n",
    "\n",
    "    attr_to_shap = {\n",
    "        attr: float(shap_values[int(idx - min_idx)])\n",
    "        for attr, idx in feature_masks[key].items()\n",
    "    }\n",
    "    print(attr_to_shap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook_env_13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
