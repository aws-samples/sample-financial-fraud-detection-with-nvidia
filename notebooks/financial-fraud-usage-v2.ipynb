{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The objective of this notebook is to showcase the usage of the [___financial-fraud-training___ container](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/cugraph/containers/financial-fraud-training) and how to deploy the produced trained models on [NVIDIA Dynamo-Triton](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver).\n",
    "- We use [IBM TabFormer](https://github.com/IBM/TabFormer) as an example dataset and the dataset is preprocess before model training\n",
    "\n",
    "NOTE:\n",
    "* The preprocessing code is written specifically for the TabFormer dataset and will not work with other datasets.\n",
    "* Additionally, a familiarity with [Jupyter](https://docs.jupyter.org/en/latest/what_is_jupyter.html) is assumed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup (Sagemaker studio)\n",
    "This Notebook is designed to work in a Sagemaker studio jupyter lab notebook\n",
    "\n",
    "Please create a Conda environment and add that to the notebook - See the [README - Setup Development Environment section](../README.md) file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:13.714496Z",
     "iopub.status.busy": "2025-09-29T16:05:13.714235Z",
     "iopub.status.idle": "2025-09-29T16:05:13.899396Z",
     "shell.execute_reply": "2025-09-29T16:05:13.898724Z",
     "shell.execute_reply.started": "2025-09-29T16:05:13.714474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA-SMI version  : 550.163.01\n",
      "NVML version        : 550.163\n",
      "DRIVER version      : 550.163.01\n",
      "CUDA Version        : 12.6\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activate the env in the kernel\n",
    "\n",
    "Now choose the `fraud_blueprint_env` kernel from within jupyterlab for this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:14.680520Z",
     "iopub.status.busy": "2025-09-29T16:05:14.680251Z",
     "iopub.status.idle": "2025-09-29T16:05:14.760961Z",
     "shell.execute_reply": "2025-09-29T16:05:14.760455Z",
     "shell.execute_reply.started": "2025-09-29T16:05:14.680496Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the AWS Environment variables from the infrastructure stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:15.255667Z",
     "iopub.status.busy": "2025-09-29T16:05:15.255406Z",
     "iopub.status.idle": "2025-09-29T16:05:15.260735Z",
     "shell.execute_reply": "2025-09-29T16:05:15.260226Z",
     "shell.execute_reply.started": "2025-09-29T16:05:15.255646Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_cfn_output(stack_name, output_key):\n",
    "    try:\n",
    "        response = cfn_client.describe_stacks(StackName=stack_name)\n",
    "        outputs = response['Stacks'][0]['Outputs']\n",
    "        for output in outputs:\n",
    "            if output['OutputKey'] == output_key:\n",
    "                return output['OutputValue']\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        if 'AccessDenied' in str(e) or 'UnauthorizedOperation' in str(e):\n",
    "            print(f\"Permission Error: You don't have permission to access CloudFormation stack '{stack_name}'.\")\n",
    "            print(\"   Please ensure the Sagemaker execution role has the 'cloudformation:DescribeStacks' permission.\")\n",
    "        else:\n",
    "            print(f\"Error accessing CloudFormation stack '{stack_name}': {str(e)}\")\n",
    "            print(\"   Please check that the stack exists and you have the permissions to read from it configured.\")\n",
    "        return None\n",
    "\n",
    "def get_inference_host():\n",
    "    try:\n",
    "        response = elb_client.describe_load_balancers()[\"LoadBalancers\"]\n",
    "        \n",
    "        for elb in response:\n",
    "            if elb['LoadBalancerName'].startswith('k8s-triton'):\n",
    "                return elb['DNSName']\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        if 'AccessDenied' in str(e) or 'UnauthorizedOperation' in str(e):\n",
    "            print(f\"Permission Error: You don't have permission to access Elastic Load Balancers.\")\n",
    "            print(\"   Please ensure the Sagemaker execution role has the 'elasticloadbalancing:DescribeLoadBalancers' permission.\")\n",
    "        else:\n",
    "            print(f\"Error accessing load balancers: {str(e)}\")\n",
    "            print(\"   Please check your AWS credentials and network connectivity.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:15.718272Z",
     "iopub.status.busy": "2025-09-29T16:05:15.718024Z",
     "iopub.status.idle": "2025-09-29T16:05:16.089144Z",
     "shell.execute_reply": "2025-09-29T16:05:16.088631Z",
     "shell.execute_reply.started": "2025-09-29T16:05:15.718250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket Name: ml-on-containers-771830474512\n",
      "Training Role: arn:aws:iam::771830474512:role/AmazonSageMaker-ExecutionRole-CDK\n",
      "Training Repo: 771830474512.dkr.ecr.us-east-1.amazonaws.com/nvidia-training-repo\n",
      "Inference Host: k8s-triton-tritonse-3c19d68b79-1481497879.us-east-1.elb.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "ssm_client = boto3.client('ssm')\n",
    "cfn_client = boto3.client('cloudformation')\n",
    "elb_client = boto3.client('elbv2', region_name=\"us-east-1\")\n",
    "\n",
    "bucket_name = get_cfn_output(\"NvidiaFraudDetectionBlueprintModelExtractor\", \"SourceBucketName\")\n",
    "\n",
    "sagemaker_training_role = get_cfn_output(\"NvidiaFraudDetectionTrainingRole\", \"SageMakerRoleArn\")\n",
    "\n",
    "training_repo = get_cfn_output(\"NvidiaFraudDetectionTrainingImageRepo\", \"TrainingImageRepoUri\")\n",
    "\n",
    "inference_host = get_inference_host()\n",
    "\n",
    "print(f\"Bucket Name: {bucket_name}\")\n",
    "print(f\"Training Role: {sagemaker_training_role}\")\n",
    "print(f\"Training Repo: {training_repo}\")\n",
    "print(f\"Inference Host: {inference_host}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Step 1: Get and Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Local\n",
    "1. Download the dataset: https://ibm.ent.box.com/v/tabformer-data/folder/130747715605\n",
    "2. untar and uncompreess the file: `tar -xvzf ./transactions.tgz`\n",
    "3. Put card_transaction.v1.csv in in the `data/TabFormer/raw` folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Brev \n",
    "1. Download the dataset: https://ibm.ent.box.com/v/tabformer-data/folder/130747715605\n",
    "2. In the Jupyter notebook window, use the \"File Browser\" section to the data/Tabformer/raw folder\n",
    "3. Drag-and-drop the \"transactions.tgz\" file into the folder\n",
    "    - There is also an \"upload\" option that displays a file selector\n",
    "    - Please wait for the upload to finish, it could take a while, by lookign at the status indocator at the bottom of the window\n",
    "4. Now uncompress and untar by running the following command\n",
    "    - Note: if somethign goes wrong you will need to delete the file rather than trying to overwrite it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:18.569094Z",
     "iopub.status.busy": "2025-09-29T16:05:18.568832Z",
     "iopub.status.idle": "2025-09-29T16:05:18.732010Z",
     "shell.execute_reply": "2025-09-29T16:05:18.731361Z",
     "shell.execute_reply.started": "2025-09-29T16:05:18.569073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2.2G\n",
      "-rw-r--r-- 1 sagemaker-user users 2.2G Oct 28  2020 card_transaction.v1.csv\n",
      "-rw-r--r-- 1 sagemaker-user users  112 Sep 26 16:24 readme.md\n"
     ]
    }
   ],
   "source": [
    "# verify that the compressed file was uploaded successfully - the size should be 266M\n",
    "!ls -lh ../data/TabFormer/raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:18.997491Z",
     "iopub.status.busy": "2025-09-29T16:05:18.997218Z",
     "iopub.status.idle": "2025-09-29T16:05:19.148346Z",
     "shell.execute_reply": "2025-09-29T16:05:19.147676Z",
     "shell.execute_reply.started": "2025-09-29T16:05:18.997467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar (child): ../data/TabFormer/raw/transactions.tgz: Cannot open: No such file or directory\n",
      "tar (child): Error is not recoverable: exiting now\n",
      "tar: Child returned status 2\n",
      "tar: Error is not recoverable: exiting now\n"
     ]
    }
   ],
   "source": [
    "# Uncompress/untar the file\n",
    "!tar xvzf ../data/TabFormer/raw/transactions.tgz -C ../data/TabFormer/raw/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__If__ drag-and-drop is not working, please run the [Download TabFormer](./extra/download-tabformer.ipynb) notebook is the \"extra\" folder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data folder structure\n",
    "The goal is to produce the following structure\n",
    "\n",
    "```\n",
    ".\n",
    "    data\n",
    "    └── TabFormer\n",
    "        └── raw\n",
    "            └── card_transaction.v1.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:21.192922Z",
     "iopub.status.busy": "2025-09-29T16:05:21.192647Z",
     "iopub.status.idle": "2025-09-29T16:05:21.196483Z",
     "shell.execute_reply": "2025-09-29T16:05:21.195971Z",
     "shell.execute_reply.started": "2025-09-29T16:05:21.192898Z"
    }
   },
   "outputs": [],
   "source": [
    "# Once the raw data is placed as described above, set the path to the TabFormer directory\n",
    "\n",
    "# Change this path to point to TabFormer data\n",
    "data_root_dir = os.path.abspath('../data/TabFormer/') \n",
    "# Change this path to the directory where you want to save your model\n",
    "model_output_dir = os.path.join(data_root_dir, 'trained_models')\n",
    "\n",
    "# Path to save the trained model\n",
    "os.makedirs(model_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define python function to print directory tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:22.114345Z",
     "iopub.status.busy": "2025-09-29T16:05:22.114089Z",
     "iopub.status.idle": "2025-09-29T16:05:22.118316Z",
     "shell.execute_reply": "2025-09-29T16:05:22.117828Z",
     "shell.execute_reply.started": "2025-09-29T16:05:22.114323Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_tree(directory, prefix=\"\"):\n",
    "    \"\"\"Recursively prints the directory tree starting at 'directory'.\"\"\"\n",
    "    # Retrieve a sorted list of entries in the directory\n",
    "    entries = sorted(os.listdir(directory))\n",
    "    entries_count = len(entries)\n",
    "    \n",
    "    for index, entry in enumerate(entries):\n",
    "        path = os.path.join(directory, entry)\n",
    "        # Determine the branch connector\n",
    "        if index == entries_count - 1:\n",
    "            connector = \"└── \"\n",
    "            extension = \"    \"\n",
    "        else:\n",
    "            connector = \"├── \"\n",
    "            extension = \"│   \"\n",
    "        \n",
    "        print(prefix + connector + entry)\n",
    "        \n",
    "        # If the entry is a directory, recursively print its contents\n",
    "        if os.path.isdir(path):\n",
    "            print_tree(path, prefix + extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:23.403832Z",
     "iopub.status.busy": "2025-09-29T16:05:23.403580Z",
     "iopub.status.idle": "2025-09-29T16:05:23.407497Z",
     "shell.execute_reply": "2025-09-29T16:05:23.407017Z",
     "shell.execute_reply.started": "2025-09-29T16:05:23.403811Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "├── gnn\n",
      "│   ├── edges\n",
      "│   │   ├── node_to_node.csv\n",
      "│   │   ├── user_to_merchant.csv\n",
      "│   │   ├── user_to_merchant_attr.csv\n",
      "│   │   └── user_to_merchant_label.csv\n",
      "│   ├── nodes\n",
      "│   │   ├── merchant.csv\n",
      "│   │   ├── node.csv\n",
      "│   │   ├── node_label.csv\n",
      "│   │   ├── offset_range_of_training_node.json\n",
      "│   │   └── user.csv\n",
      "│   └── test_gnn\n",
      "│       ├── edges\n",
      "│       │   ├── node_to_node.csv\n",
      "│       │   ├── user_to_merchant.csv\n",
      "│       │   ├── user_to_merchant_attr.csv\n",
      "│       │   ├── user_to_merchant_feature_mask.csv\n",
      "│       │   └── user_to_merchant_label.csv\n",
      "│       └── nodes\n",
      "│           ├── merchant.csv\n",
      "│           ├── merchant_feature_mask.csv\n",
      "│           ├── node.csv\n",
      "│           ├── node_label.csv\n",
      "│           ├── user.csv\n",
      "│           └── user_feature_mask.csv\n",
      "├── raw\n",
      "│   ├── card_transaction.v1.csv\n",
      "│   └── readme.md\n",
      "├── trained_models\n",
      "└── xgb\n",
      "    ├── test.csv\n",
      "    ├── training.csv\n",
      "    └── validation.csv\n"
     ]
    }
   ],
   "source": [
    "# Check if the raw data has been placed properly\n",
    "print_tree(data_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 2: Preprocess the data \n",
    "- Import the Python function for preprocessing the TabFormer data\n",
    "- Call `preprocess_TabFormer` function to prepare the data\n",
    "\n",
    "NOTE: The preprocessing can takes a few minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:27.535005Z",
     "iopub.status.busy": "2025-09-29T16:05:27.534753Z",
     "iopub.status.idle": "2025-09-29T16:05:29.208030Z",
     "shell.execute_reply": "2025-09-29T16:05:29.207521Z",
     "shell.execute_reply.started": "2025-09-29T16:05:27.534985Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add the \"src\" directory to the search path\n",
    "src_dir = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "sys.path.insert(0, src_dir)\n",
    "\n",
    "# should be able to import from \"src\" folder now\n",
    "from preprocess_TabFormer_lp import preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:29.209127Z",
     "iopub.status.busy": "2025-09-29T16:05:29.208801Z",
     "iopub.status.idle": "2025-09-29T16:07:17.187411Z",
     "shell.execute_reply": "2025-09-29T16:07:17.186820Z",
     "shell.execute_reply.started": "2025-09-29T16:05:29.209108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation (Card, Fraud) =   6.59%\n",
      "Correlation (Chip, Fraud) =   5.63%\n",
      "Correlation (Errors, Fraud) =   1.81%\n",
      "Correlation (State, Fraud) =  35.92%\n",
      "Correlation (City, Fraud) =  32.47%\n",
      "Correlation (Zip, Fraud) =  14.99%\n",
      "Correlation (MCC, Fraud) =  12.70%\n",
      "Correlation (Merchant, Fraud) =  34.88%\n",
      "Correlation (User, Fraud) =   3.40%\n",
      "Correlation (Day, Fraud) =   0.26%\n",
      "Correlation (Month, Fraud) =   0.23%\n",
      "Correlation (Year, Fraud) =   2.35%\n",
      "r_pb (Time) = -0.00 with p_value 0.00\n",
      "r_pb (Amount) = 0.03 with p_value 0.00\n",
      "Transaction ID range (np.int64(0), np.int64(301523))\n",
      "Merchant ID range (np.int64(0), np.int64(42941))\n",
      "User ID range (np.int64(0), np.int64(4872))\n",
      "Transaction ID range (np.int64(0), np.int64(25802))\n",
      "Merchant ID range (np.int64(0), np.int64(6772))\n",
      "User ID range (np.int64(0), np.int64(4794))\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "user_mask_map, mx_mask_map, tx_mask_map = preprocess_data(data_root_dir)\n",
    "\n",
    "# this will output status as it correlates different attributes with target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:07:17.188254Z",
     "iopub.status.busy": "2025-09-29T16:07:17.188059Z",
     "iopub.status.idle": "2025-09-29T16:07:17.191760Z",
     "shell.execute_reply": "2025-09-29T16:07:17.191324Z",
     "shell.execute_reply.started": "2025-09-29T16:07:17.188236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "├── gnn\n",
      "│   ├── edges\n",
      "│   │   ├── node_to_node.csv\n",
      "│   │   ├── user_to_merchant.csv\n",
      "│   │   ├── user_to_merchant_attr.csv\n",
      "│   │   └── user_to_merchant_label.csv\n",
      "│   ├── nodes\n",
      "│   │   ├── merchant.csv\n",
      "│   │   ├── node.csv\n",
      "│   │   ├── node_label.csv\n",
      "│   │   ├── offset_range_of_training_node.json\n",
      "│   │   └── user.csv\n",
      "│   └── test_gnn\n",
      "│       ├── edges\n",
      "│       │   ├── node_to_node.csv\n",
      "│       │   ├── user_to_merchant.csv\n",
      "│       │   ├── user_to_merchant_attr.csv\n",
      "│       │   ├── user_to_merchant_feature_mask.csv\n",
      "│       │   └── user_to_merchant_label.csv\n",
      "│       └── nodes\n",
      "│           ├── merchant.csv\n",
      "│           ├── merchant_feature_mask.csv\n",
      "│           ├── node.csv\n",
      "│           ├── node_label.csv\n",
      "│           ├── user.csv\n",
      "│           └── user_feature_mask.csv\n",
      "├── raw\n",
      "│   ├── card_transaction.v1.csv\n",
      "│   └── readme.md\n",
      "├── trained_models\n",
      "└── xgb\n",
      "    ├── test.csv\n",
      "    ├── training.csv\n",
      "    └── validation.csv\n"
     ]
    }
   ],
   "source": [
    "print_tree(data_root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:07:17.192751Z",
     "iopub.status.busy": "2025-09-29T16:07:17.192511Z",
     "iopub.status.idle": "2025-09-29T16:07:23.623610Z",
     "shell.execute_reply": "2025-09-29T16:07:23.622894Z",
     "shell.execute_reply.started": "2025-09-29T16:07:17.192733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../data/TabFormer/gnn/edges/user_to_merchant.csv to s3://ml-on-containers-771830474512/data/gnn/edges/user_to_merchant.csv\n",
      "upload: ../data/TabFormer/gnn/nodes/user.csv to s3://ml-on-containers-771830474512/data/gnn/nodes/user.csv\n",
      "upload: ../data/TabFormer/gnn/edges/user_to_merchant_label.csv to s3://ml-on-containers-771830474512/data/gnn/edges/user_to_merchant_label.csv\n",
      "upload: ../data/TabFormer/gnn/nodes/merchant.csv to s3://ml-on-containers-771830474512/data/gnn/nodes/merchant.csv\n",
      "upload: ../data/TabFormer/gnn/test_gnn/edges/user_to_merchant_feature_mask.csv to s3://ml-on-containers-771830474512/data/gnn/test_gnn/edges/user_to_merchant_feature_mask.csv\n",
      "upload: ../data/TabFormer/gnn/test_gnn/edges/user_to_merchant_attr.csv to s3://ml-on-containers-771830474512/data/gnn/test_gnn/edges/user_to_merchant_attr.csv\n",
      "upload: ../data/TabFormer/gnn/test_gnn/edges/user_to_merchant.csv to s3://ml-on-containers-771830474512/data/gnn/test_gnn/edges/user_to_merchant.csv\n",
      "upload: ../data/TabFormer/gnn/test_gnn/edges/user_to_merchant_label.csv to s3://ml-on-containers-771830474512/data/gnn/test_gnn/edges/user_to_merchant_label.csv\n",
      "upload: ../data/TabFormer/gnn/test_gnn/nodes/merchant_feature_mask.csv to s3://ml-on-containers-771830474512/data/gnn/test_gnn/nodes/merchant_feature_mask.csv\n",
      "upload: ../data/TabFormer/gnn/test_gnn/nodes/user_feature_mask.csv to s3://ml-on-containers-771830474512/data/gnn/test_gnn/nodes/user_feature_mask.csv\n",
      "upload: ../data/TabFormer/gnn/test_gnn/nodes/merchant.csv to s3://ml-on-containers-771830474512/data/gnn/test_gnn/nodes/merchant.csv\n",
      "upload: ../data/TabFormer/gnn/test_gnn/nodes/user.csv to s3://ml-on-containers-771830474512/data/gnn/test_gnn/nodes/user.csv\n",
      "upload: ../data/TabFormer/xgb/test.csv to s3://ml-on-containers-771830474512/data/xgb/test.csv\n",
      "upload: ../data/TabFormer/xgb/validation.csv to s3://ml-on-containers-771830474512/data/xgb/validation.csv\n",
      "upload: ../data/TabFormer/gnn/edges/user_to_merchant_attr.csv to s3://ml-on-containers-771830474512/data/gnn/edges/user_to_merchant_attr.csv\n",
      "upload: ../data/TabFormer/xgb/training.csv to s3://ml-on-containers-771830474512/data/xgb/training.csv\n"
     ]
    }
   ],
   "source": [
    "# copy data to S3 to get pulled during training\n",
    "! aws s3 sync \"../data/TabFormer/\" s3://$bucket_name/data/ --exclude \"*/test_gnn\" --force --delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Step 3:  Now train the model using the financial-fraud-training container\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training configuration file\n",
    "NOTE: Training configuration file must conform to schema defined in docs (to be updated.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Important: Models and configuration files needed for deployment using NVIDIA Dynamo-Triton will be saved in model-repository under the folder that is mounted in /trained_models inside the container__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:09:52.679791Z",
     "iopub.status.busy": "2025-09-29T16:09:52.679511Z",
     "iopub.status.idle": "2025-09-29T16:09:52.683485Z",
     "shell.execute_reply": "2025-09-29T16:09:52.682940Z",
     "shell.execute_reply.started": "2025-09-29T16:09:52.679764Z"
    }
   },
   "outputs": [],
   "source": [
    "training_config = {\n",
    "  \"paths\": {\n",
    "        \"data_dir\": \"/opt/ml/input/data/data\",\n",
    "        \"output_dir\": \"/opt/ml/model\"\n",
    "  },\n",
    "\n",
    "  \"models\": [\n",
    "    {\n",
    "      \"kind\": \"GNN_XGBoost\",\n",
    "      \"gpu\": \"single\",\n",
    "      \"hyperparameters\": {\n",
    "        \"gnn\":{\n",
    "          \"hidden_channels\": 32,\n",
    "          \"n_hops\": 2,\n",
    "          \"layer\": \"SAGEConv\",\n",
    "          \"dropout_prob\": 0.1,\n",
    "          \"batch_size\": 4096,\n",
    "          \"fan_out\": 10,\n",
    "          \"num_epochs\": 1\n",
    "        },\n",
    "        \"xgb\": {\n",
    "          \"max_depth\": 6,\n",
    "          \"learning_rate\": 0.2,\n",
    "          \"num_parallel_tree\": 3,\n",
    "          \"num_boost_round\": 512,\n",
    "          \"gamma\": 0.0\n",
    "        }\n",
    "\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the training configuration as a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:09:53.598930Z",
     "iopub.status.busy": "2025-09-29T16:09:53.598668Z",
     "iopub.status.idle": "2025-09-29T16:09:54.580158Z",
     "shell.execute_reply": "2025-09-29T16:09:54.579510Z",
     "shell.execute_reply.started": "2025-09-29T16:09:53.598908Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./training_config.json to s3://ml-on-containers-771830474512/config/training_config.json\n"
     ]
    }
   ],
   "source": [
    "training_config_file_name = 'training_config.json'\n",
    "\n",
    "with open(os.path.join(training_config_file_name), 'w') as json_file:\n",
    "    json.dump(training_config, json_file, indent=4)\n",
    "\n",
    "# clone config to S3\n",
    "! aws s3 cp ./training_config.json s3://$bucket_name/config/training_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model using financial_fraud_training container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:50:16.411615Z",
     "iopub.status.busy": "2025-09-29T16:50:16.411338Z",
     "iopub.status.idle": "2025-09-29T16:50:17.029736Z",
     "shell.execute_reply": "2025-09-29T16:50:17.029201Z",
     "shell.execute_reply.started": "2025-09-29T16:50:16.411593Z"
    }
   },
   "outputs": [],
   "source": [
    "# run the training in Sagemaker\n",
    "# define training job\n",
    "training_job_name = \"fraud-detection-gnn-\" + str(int(time.time()))\n",
    "\n",
    "# send training job to parameter store\n",
    "ssm_client = boto3.client(\"ssm\")\n",
    "\n",
    "ssm_client.put_parameter(Name=\"/triton/model\", Value=training_job_name, Overwrite=True)\n",
    "\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "response = sagemaker_client.create_training_job(\n",
    "    TrainingJobName=training_job_name,\n",
    "    RoleArn=sagemaker_training_role,\n",
    "    AlgorithmSpecification={\n",
    "        'TrainingImage': f\"{training_repo}:2.0.0\",\n",
    "        'TrainingInputMode': 'File',\n",
    "        'ContainerEntrypoint': [\n",
    "            'torchrun',\n",
    "            '--standalone',\n",
    "            '--nnodes=1',\n",
    "            '--nproc_per_node=1',\n",
    "            '/app/main.py',\n",
    "            '--config',\n",
    "            '/opt/ml/input/data/config/training_config.json'\n",
    "        ]\n",
    "    },\n",
    "    InputDataConfig=[\n",
    "        {\n",
    "            'ChannelName': 'data',\n",
    "            'DataSource': {\n",
    "                'S3DataSource': {\n",
    "                    'S3DataType': 'S3Prefix',\n",
    "                    'S3Uri': f's3://{bucket_name}/data/gnn/',\n",
    "                    'S3DataDistributionType': 'FullyReplicated'\n",
    "                }\n",
    "            },\n",
    "            'ContentType': 'application/x-directory',\n",
    "            'InputMode': 'File',\n",
    "            'CompressionType': 'None'\n",
    "        },\n",
    "        {\n",
    "            'ChannelName': 'config',\n",
    "            'DataSource': {\n",
    "                'S3DataSource': {\n",
    "                    'S3DataType': 'S3Prefix',\n",
    "                    'S3Uri': f's3://{bucket_name}/config/',\n",
    "                    'S3DataDistributionType': 'FullyReplicated',\n",
    "                }\n",
    "            },\n",
    "            'ContentType': 'application/x-directory',\n",
    "            'InputMode': 'File',\n",
    "            'CompressionType': 'None'\n",
    "        }\n",
    "    ],\n",
    "    OutputDataConfig={\n",
    "        'S3OutputPath': f's3://{bucket_name}/output/'\n",
    "    },\n",
    "    ResourceConfig={\n",
    "        'InstanceType': 'ml.g5.xlarge',\n",
    "        'InstanceCount': 1,\n",
    "        'VolumeSizeInGB': 30\n",
    "    },\n",
    "    StoppingCondition={\n",
    "        'MaxRuntimeInSeconds': 86400\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure that the training job succeeds\n",
    "According to the training configuration file defined earlier, if the training runs successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:50:17.093721Z",
     "iopub.status.busy": "2025-09-29T16:50:17.093474Z",
     "iopub.status.idle": "2025-09-29T16:50:17.097472Z",
     "shell.execute_reply": "2025-09-29T16:50:17.097021Z",
     "shell.execute_reply.started": "2025-09-29T16:50:17.093700Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "def poll_training_status(job_name):\n",
    "    sagemaker_client = boto3.client('sagemaker')\n",
    "    \n",
    "    while True:\n",
    "        response = sagemaker_client.describe_training_job(TrainingJobName=job_name)\n",
    "        status = response['TrainingJobStatus']\n",
    "        \n",
    "        # Clear previous output\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # Print current status with timestamp\n",
    "        print(f\"Job: {job_name}\")\n",
    "        print(f\"Status: {status}\")\n",
    "        print(f\"Last checked: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        if status in ['Completed', 'Failed', 'Stopped']:\n",
    "            break\n",
    "            \n",
    "        # Wait before next check\n",
    "        time.sleep(30)\n",
    "    \n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:50:17.257552Z",
     "iopub.status.busy": "2025-09-29T16:50:17.257309Z",
     "iopub.status.idle": "2025-09-29T16:59:48.463899Z",
     "shell.execute_reply": "2025-09-29T16:59:48.463412Z",
     "shell.execute_reply.started": "2025-09-29T16:50:17.257533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job: fraud-detection-gnn-1759164616\n",
      "Status: Failed\n",
      "Last checked: 2025-09-29 16:59:48\n"
     ]
    }
   ],
   "source": [
    "final_status = poll_training_status(training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Step 4:  Serve your python backend model using NVIDIA Dynamo-Triton\n",
    "__!Important__: Change MODEL_REPO_PATH to point to `{model_output_dir}` / `python_backend_model_repository` if you used a different path in your training configuration file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install NVIDIA Dynamo-Triton Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'tritonclient[all]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.grpc as triton_grpc\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient import utils as triton_utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Replace HOST with the actual URL where your NVIDIA Dynamo-Triton server is hosted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = inference_host\n",
    "HTTP_PORT = 8005\n",
    "GRPC_PORT = 8006\n",
    "METRICS_PORT = 8007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serve your models with NVIDIA Dynamo-Triton\n",
    "\n",
    "With the infrastructure repo deployed, we have a Lambda function waiting for the training job to complete and for the models to be output to `s3://ml-on-containers-<accountnumber>/output` and then, they'll get extracted to a different bucket to be served by the inference host setup by our infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URLs for GRPC and HTTP request to the inference server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_grpc = triton_grpc.InferenceServerClient(url=f'{HOST}:{GRPC_PORT}')\n",
    "client_http = httpclient.InferenceServerClient(url=f'{HOST}:{HTTP_PORT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for NVIDIA Dynamo-Triton to install packages and come online\n",
    "**NOTE**: This cell can take a few minutes to execute.\n",
    " If the following cell keeps running even after you see `Started HTTPService at {HOST}:{HTTP_PORT}` in the log, you can interrupt the execution of this cell and continue from the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "while True:\n",
    "    client_grpc = triton_grpc.InferenceServerClient(url=f'{HOST}:{GRPC_PORT}')\n",
    "    try:\n",
    "        if client_grpc.is_server_ready():\n",
    "            break\n",
    "    except triton_utils.InferenceServerException as e:\n",
    "        pass\n",
    "    try:\n",
    "        # Run the docker logs command with the --tail option\n",
    "        output = \"TODO: Figure out what's going on here and how to display container logs from k8s inference host\"\n",
    "        print(output.decode(\"utf-8\"))\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error retrieving logs:\", e)\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if NVIDIA Dynamo-Triton is running properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Figure out how to demonstrate that the pod is online from our container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here’s an example of how to prepare data for inference, using random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tritonclient.http import InferenceServerClient, InferInput, InferRequestedOutput\n",
    "\n",
    "def make_example_request():\n",
    "    # -- example sizes --\n",
    "    num_merchants = 5\n",
    "    num_users   = 7\n",
    "    num_edges   = 3\n",
    "    merchant_feature_dim = 24\n",
    "    user_feature_dim = 13\n",
    "    user_to_merchant_feature_dim = 38\n",
    "\n",
    "    # -- 1) features --\n",
    "    x_merchant = np.random.randn(num_merchants, merchant_feature_dim).astype(np.float32)\n",
    "    x_user   = np.random.randn(num_users, user_feature_dim).astype(np.float32)\n",
    "\n",
    "    # -- 2) shap flag and masks --\n",
    "    compute_shap          = np.array([True], dtype=np.bool_)\n",
    "    feature_mask_merchant   = np.random.randint(0,2, size=(merchant_feature_dim,), dtype=np.int32)\n",
    "    feature_mask_user     = np.random.randint(0,2, size=(user_feature_dim,), dtype=np.int32)\n",
    "\n",
    "    # -- 3) edges: index [2, num_edges] and attributes [num_edges,user_to_merchant_feature_dim] --\n",
    "    edge_index_user_to_merchant = np.vstack([\n",
    "        np.random.randint(0, num_users,   size=(num_edges,)),\n",
    "        np.random.randint(0, num_merchants, size=(num_edges,))\n",
    "    ]).astype(np.int64)\n",
    "    \n",
    "    edge_attr_user_to_merchant = np.random.randn(num_edges, user_to_merchant_feature_dim).astype(np.float32)\n",
    "\n",
    "    feature_mask_user_to_merchant =  np.random.randint(0,2, size=(user_to_merchant_feature_dim,), dtype=np.int32)\n",
    "\n",
    "    return {\n",
    "        \"x_merchant\": x_merchant,\n",
    "        \"x_user\": x_user,\n",
    "        \"COMPUTE_SHAP\": compute_shap,\n",
    "        \"feature_mask_merchant\": feature_mask_merchant,\n",
    "        \"feature_mask_user\": feature_mask_user,\n",
    "        \"edge_index_user_to_merchant\": edge_index_user_to_merchant,\n",
    "        \"edge_attr_user_to_merchant\": edge_attr_user_to_merchant,\n",
    "        \"edge_feature_mask_user_to_merchant\": feature_mask_user_to_merchant\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_and_send_inference_request(data):\n",
    "\n",
    "    # Connect to Triton\n",
    "    client = httpclient.InferenceServerClient(url=f'{HOST}:{HTTP_PORT}')\n",
    "\n",
    "    # Prepare Inputs\n",
    "\n",
    "    inputs = []\n",
    "    def _add_input(name, arr, dtype):\n",
    "        inp = InferInput(name, arr.shape, datatype=dtype)\n",
    "        inp.set_data_from_numpy(arr)\n",
    "        inputs.append(inp)\n",
    "\n",
    "    for key, value in data.items():\n",
    "        if key.startswith(\"x_\"):\n",
    "            dtype = \"FP32\"\n",
    "        elif key.startswith(\"feature_mask_\"):\n",
    "            dtype = \"INT32\"\n",
    "        elif key.startswith(\"edge_feature_mask_\"):\n",
    "            dtype = \"INT32\"            \n",
    "        elif key.startswith(\"edge_index_\"):\n",
    "            dtype = \"INT64\"\n",
    "        elif key.startswith(\"edge_attr_\"):\n",
    "            dtype = \"FP32\"\n",
    "        elif key == \"COMPUTE_SHAP\":\n",
    "            dtype = \"BOOL\"\n",
    "        else:\n",
    "            continue  # skip things we don't care about\n",
    "\n",
    "        _add_input(key, value, dtype)\n",
    "\n",
    "\n",
    "    # Outputs\n",
    "\n",
    "    outputs = [InferRequestedOutput(\"PREDICTION\")]\n",
    "\n",
    "    for key in data:\n",
    "        if key.startswith(\"x_\"):\n",
    "            node = key[len(\"x_\"):]  # extract node name\n",
    "            outputs.append(InferRequestedOutput(f\"shap_values_{node}\"))\n",
    "        elif key.startswith(\"edge_attr_\"):\n",
    "            edge_name = key[len(\"edge_attr_\"):]  # extract edge name\n",
    "            outputs.append(InferRequestedOutput(f\"shap_values_{edge_name}\"))\n",
    "    \n",
    "    # Send request\n",
    "\n",
    "    model_name=\"prediction_and_shapley\"\n",
    "    response = client.infer(\n",
    "        model_name,\n",
    "        inputs=inputs,\n",
    "        request_id=str(1),\n",
    "        outputs=outputs,\n",
    "        timeout= 3000\n",
    "    )\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    # always include prediction\n",
    "    result[\"PREDICTION\"] = response.as_numpy(\"PREDICTION\")\n",
    "\n",
    "    # add shap values\n",
    "    for key in data:\n",
    "        if key.startswith(\"x_\"):\n",
    "            node = key[len(\"x_\"):]  # e.g. \"merchant\", \"user\"\n",
    "            result[f\"shap_values_{node}\"] = response.as_numpy(f\"shap_values_{node}\")\n",
    "        if key.startswith(\"edge_attr_\"):\n",
    "            edge_name = key[len(\"edge_attr_\"):]  # e.g. (\"user\" \"to\"  \"merchant\")\n",
    "            result[f\"shap_values_{edge_name}\"] = response.as_numpy(f\"shap_values_{edge_name}\")\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction without computing Shapley values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read preprocessed input transactions to send query to NVIDIA Dynamo-Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_hetero_graph(gnn_data_dir):\n",
    "    \"\"\"\n",
    "    Reads:\n",
    "      - All node CSVs from nodes/, plus their matching feature masks (<node>_feature_mask.csv)\n",
    "        If missing, a mask of all ones is created (np.int32).\n",
    "      - All edge CSVs from edges/:\n",
    "          base        -> edge_index_<edge> (np.int64)\n",
    "          *_attr.csv  -> edge_attr_<edge>  (np.float32)\n",
    "          *_label.csv -> exactly one -> edge_label_<edge> (DataFrame)\n",
    "    \"\"\"\n",
    "    base = os.path.join(gnn_data_dir, \"test_gnn\")\n",
    "    nodes_dir = os.path.join(base, \"nodes\")\n",
    "    edges_dir = os.path.join(base, \"edges\")\n",
    "\n",
    "    out = {}\n",
    "    node_feature_mask = {}\n",
    "\n",
    "    # --- Nodes: every CSV becomes x_<node>; also read/create feature_mask_<node> ---\n",
    "    if os.path.isdir(nodes_dir):\n",
    "        for fname in os.listdir(nodes_dir):\n",
    "            if fname.lower().endswith(\".csv\") and not fname.lower().endswith(\"_feature_mask.csv\"):\n",
    "                node_name = fname[:-len(\".csv\")]\n",
    "                node_path = os.path.join(nodes_dir, fname)\n",
    "                node_df = pd.read_csv(node_path)\n",
    "                out[f\"x_{node_name}\"] = node_df.to_numpy(dtype=np.float32)\n",
    "\n",
    "                # feature mask file (optional)\n",
    "                mask_fname = f\"{node_name}_feature_mask.csv\"\n",
    "                mask_path = os.path.join(nodes_dir, mask_fname)\n",
    "                if os.path.exists(mask_path):\n",
    "                    mask_df = pd.read_csv(mask_path, header=None)\n",
    "                    node_feature_mask[node_name] = mask_df\n",
    "                    feature_mask = mask_df.to_numpy(dtype=np.int32).ravel()\n",
    "                else:\n",
    "                    # create a must with all zeros\n",
    "                    feature_mask = np.zeros(node_df.shape[1], dtype=np.int32)\n",
    "                out[f\"feature_mask_{node_name}\"] = feature_mask\n",
    "\n",
    "    # --- Edges: group into base, attr, label by filename suffix ---\n",
    "    base_edges = {}\n",
    "    edge_attrs = {}\n",
    "    edge_labels = {}\n",
    "    edge_feature_mask = {}\n",
    "\n",
    "    if os.path.isdir(edges_dir):\n",
    "        for fname in os.listdir(edges_dir):\n",
    "            if not fname.lower().endswith(\".csv\"):\n",
    "                continue\n",
    "            path = os.path.join(edges_dir, fname)\n",
    "            lower = fname.lower()\n",
    "            if lower.endswith(\"_attr.csv\"):\n",
    "                edge_name = fname[:-len(\"_attr.csv\")]\n",
    "                edge_attrs[edge_name] = pd.read_csv(path) #, header=None)\n",
    "            elif lower.endswith(\"_label.csv\"):\n",
    "                edge_name = fname[:-len(\"_label.csv\")]\n",
    "                edge_labels[edge_name] = pd.read_csv(path)\n",
    "            elif lower.endswith(\"_feature_mask.csv\"):\n",
    "                edge_name = fname[:-len(\"_feature_mask.csv\")]\n",
    "                edge_feature_mask[edge_name] = pd.read_csv(path, header=None)\n",
    "            else:\n",
    "                edge_name = fname[:-len(\".csv\")]\n",
    "                base_edges[edge_name] = pd.read_csv(path) #, header=None)\n",
    "\n",
    "\n",
    "\n",
    "    # Enforce: only one label file total\n",
    "    if len(edge_labels) == 0:\n",
    "        raise FileNotFoundError(\"No '*_label.csv' found in edges/. Exactly one label file is required.\")\n",
    "    if len(edge_labels) > 1:\n",
    "        raise ValueError(f\"Found multiple label files: {list(edge_labels.keys())}. Exactly one is allowed.\")\n",
    "\n",
    "    # Build output keys for edges\n",
    "    for edge_name, df in base_edges.items():\n",
    "        out[f\"edge_index_{edge_name}\"] = df.to_numpy(dtype=np.int64).T\n",
    "        if edge_name in edge_attrs:\n",
    "            out[f\"edge_attr_{edge_name}\"] = edge_attrs[edge_name].to_numpy(dtype=np.float32)\n",
    "        if edge_name in edge_feature_mask:\n",
    "            out[f\"edge_feature_mask_{edge_name}\"] = edge_feature_mask[edge_name].to_numpy(dtype=np.int32).ravel()\n",
    "        else:\n",
    "            # create a must with all zeros\n",
    "            out[f\"edge_feature_mask_{edge_name}\"] = np.zeros(edge_attrs[edge_name].shape[1], dtype=np.int32)\n",
    "\n",
    "        \n",
    "\n",
    "    # Add the single label file (kept as DataFrame)\n",
    "    (label_edge_name, label_df), = edge_labels.items()\n",
    "    out[f\"edge_label_{label_edge_name}\"] = label_df\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gnn_data_dir = os.path.join(data_root_dir, \"gnn\")\n",
    "test_data = load_hetero_graph(gnn_data_dir)\n",
    "compute_shap = False\n",
    "result =  prepare_and_send_inference_request(test_data | {\"COMPUTE_SHAP\": np.array([compute_shap], dtype=np.bool_)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['PREDICTION']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_score_for_batch(y, predictions, decision_threshold = 0.5):\n",
    "    # Apply threshold\n",
    "    y_pred = (predictions > decision_threshold).astype(int)\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred, zero_division=0)\n",
    "    recall = recall_score(y, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y, y_pred, zero_division=0)\n",
    "\n",
    "    # Confusion matrix\n",
    "    classes = ['Non-Fraud', 'Fraud']\n",
    "    columns = pd.MultiIndex.from_product([[\"Predicted\"], classes])\n",
    "    index = pd.MultiIndex.from_product([[\"Actual\"], classes])\n",
    "\n",
    "    conf_mat = confusion_matrix(y, y_pred)\n",
    "    cm_df = pd.DataFrame(conf_mat, index=index, columns=columns)\n",
    "    print(cm_df)\n",
    "\n",
    "    # Plot the confusion matrix directly\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(\n",
    "        y, y_pred, display_labels=classes\n",
    "    )\n",
    "    disp.ax_.set_title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary\n",
    "    print(\"----Summary---\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scores on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision threshold to flag a transaction as fraud\n",
    "#Change to trade-off precision and recall\n",
    "decision_threshold = 0.5\n",
    "y = test_data['edge_label_user_to_merchant'].to_numpy(dtype=np.int32)\n",
    "compute_score_for_batch(y, result['PREDICTION'], decision_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Shapley values of different features for a transaction\n",
    "NOTE: Shapely computation is very expensive, it will only compute shap values for first transaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shapley values for different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data =  load_hetero_graph(gnn_data_dir)\n",
    "compute_shap = True\n",
    "result_with_shap = prepare_and_send_inference_request(test_data | {\"COMPUTE_SHAP\": np.array([compute_shap], dtype=np.bool_)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in result_with_shap:\n",
    "    if key.startswith('shap_'):\n",
    "        print(f'{key} : {result_with_shap[key]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_masks = {\n",
    "    'user': user_mask_map,\n",
    "    'merchant': mx_mask_map,\n",
    "    'user_to_merchant': tx_mask_map\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in feature_masks:\n",
    "    shap_values = result_with_shap[f'shap_values_{key}']\n",
    "    min_idx = min(feature_masks[key].values())\n",
    "\n",
    "    attr_to_shap = {\n",
    "        attr: float(shap_values[int(idx - min_idx)])\n",
    "        for attr, idx in feature_masks[key].items()\n",
    "    }\n",
    "    print(attr_to_shap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "user-env:(fraud_blueprint_env)",
   "language": "python",
   "name": "fraud_blueprint_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
